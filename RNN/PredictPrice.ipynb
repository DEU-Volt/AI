{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "_bp1ppCa3Ssd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "data_csv = pd.read_csv('train_data.csv')\n",
    "titles_csv = data_csv['title']\n",
    "prices_csv = data_csv['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from konlpy.tag import Okt\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "try:\n",
    "    with open(\"titles_words.bin\", \"rb\") as f:\n",
    "        titles_words = pickle.load(f)\n",
    "    with open(\"dictionary.bin\", \"rb\") as f:\n",
    "        dictionary = pickle.load(f)\n",
    "    with open(\"titles_ids.bin\", \"rb\") as f:\n",
    "        titles_ids = pickle.load(f)\n",
    "        \n",
    "except Exception as e:\n",
    "    okt = Okt()\n",
    "    words_set = set()\n",
    "    titles_words = []\n",
    "    count = 1\n",
    "    for title in titles_csv:\n",
    "        title_pos = okt.pos(title, norm=True)\n",
    "        words = []\n",
    "        for word in title_pos:\n",
    "            words_set.add(word[0])\n",
    "            words.append(word[0])\n",
    "        titles_words.append(words)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{count} / {len(titles_csv)}\")\n",
    "        count += 1\n",
    "        \n",
    "    dictionary = list(words_set)\n",
    "    random.shuffle(dictionary)\n",
    "    titles_ids = []\n",
    "    count = 1\n",
    "    for title in titles_words:\n",
    "        words_id = []\n",
    "        for words in title:\n",
    "            words_id.append(dictionary.index(words))\n",
    "        titles_ids.append(words_id)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{count} / {len(titles_words)}\")\n",
    "        count += 1\n",
    "        \n",
    "    with open(\"titles_words.bin\", \"wb\") as f:\n",
    "        pickle.dump(titles_words, f)\n",
    "    with open(\"dictionary.bin\", \"wb\") as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "    with open(\"titles_ids.bin\", \"wb\") as f:\n",
    "        pickle.dump(titles_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', '8', '부품', '용', '무선', '충전', '패드']\n",
      "[2766, 1235, 23, 5052, 702, 3126, 2617]\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "print(titles_words[0])\n",
    "print(titles_ids[0])\n",
    "print(dictionary[titles_ids[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(title_ids) for title_ids in titles_ids)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2766 1235   23 ...    0    0    0]\n",
      " [5763 1175 2898 ...    0    0    0]\n",
      " [1606 1820 5297 ...    0    0    0]\n",
      " ...\n",
      " [5763 1175 6402 ...    0    0    0]\n",
      " [1606 5667 3915 ...    0    0    0]\n",
      " [4686 1922 1477 ...    0    0    0]]\n",
      "[[ 10000]\n",
      " [ 10000]\n",
      " [ 10000]\n",
      " ...\n",
      " [ 50000]\n",
      " [200000]\n",
      " [270000]]\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "\n",
    "titles_ids_np = sequence.pad_sequences(titles_ids, maxlen=max_len, padding='post')\n",
    "print(titles_ids_np)\n",
    "\n",
    "prices_np = np.array([[price] for price in prices_csv])\n",
    "print(prices_np)\n",
    "\n",
    "print(type(titles_ids_np), type(prices_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52948\n",
      "42358\n",
      "10590\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = [i for i in range(len(titles_ids_np))]\n",
    "random.shuffle(index)\n",
    "\n",
    "train_len = int(len(titles_ids_np) * 0.8)\n",
    "train_index = index[:train_len]\n",
    "test_index = index[train_len:]\n",
    "\n",
    "print(len(titles_ids_np))\n",
    "print(len(train_index))\n",
    "print(len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1606 1235    0 ...    0    0    0]\n",
      " [5763 1175 6402 ...    0    0    0]\n",
      " [3294 7895 4885 ...    0    0    0]\n",
      " ...\n",
      " [5763 1175 4284 ...    0    0    0]\n",
      " [5763 2766 3481 ...    0    0    0]\n",
      " [5763 2766 2766 ...    0    0    0]]\n",
      "[[240000]\n",
      " [610000]\n",
      " [ 60000]\n",
      " ...\n",
      " [320000]\n",
      " [ 15000]\n",
      " [220000]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train = titles_ids_np[train_index]\n",
    "X_test = titles_ids_np[test_index]\n",
    "\n",
    "y_test = prices_np[test_index]\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(prices_np)\n",
    "mmt_y = mms.transform(prices_np)\n",
    "\n",
    "mmt_y_train = mmt_y[train_index]\n",
    "mmt_y_test = mmt_y[test_index]\n",
    "\n",
    "# ss = StandardScaler()\n",
    "# ss.fit(prices_np)\n",
    "# st_y = ss.transform(prices_np)\n",
    "\n",
    "# st_y_train = st_y[train_index]\n",
    "# st_y_test = st_y[test_index]\n",
    "\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model  Parameter Tunning \n",
    "##### 1. Epoch 변경\n",
    "##### 2. Activation 변경\n",
    "##### 3. Optimizer 변경\n",
    "##### 4. Dropout layer 추가 및 변경\n",
    "##### 5. GRU, LSTM layer 추가 및 변경 (, return_sequences=True 변경)\n",
    "##### 6. Embed size 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "5ahQUo6qX3oP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 128)         1076480   \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, None, 128)         99072     \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,274,753\n",
      "Trainable params: 1,274,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = len(dictionary)\n",
    "embed_size = 128\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embed_size, input_shape=[None]),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    # tf.keras.layers.GRU(64),\n",
    "    # tf.keras.layers.GRU(32),\n",
    "    # tf.keras.layers.GRU(16),\n",
    "    # tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1)\n",
    "    # tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=\"adam\", metrics=[\"mae\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1324/1324 [==============================] - 28s 19ms/step - loss: 1.9213e-04 - mae: 0.0071\n",
      "Epoch 2/5\n",
      "1324/1324 [==============================] - 27s 20ms/step - loss: 2.4922e-05 - mae: 0.0027\n",
      "Epoch 3/5\n",
      "1324/1324 [==============================] - 27s 21ms/step - loss: 6.0037e-05 - mae: 0.0025\n",
      "Epoch 4/5\n",
      "1324/1324 [==============================] - 27s 21ms/step - loss: 5.4616e-05 - mae: 0.0025\n",
      "Epoch 5/5\n",
      "1324/1324 [==============================] - 28s 21ms/step - loss: 4.1519e-05 - mae: 0.0023\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, mmt_y_train, epochs=5, verbose=1)\n",
    "# model.save(\"PredictPrice_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[240000.       160806.734375]\n",
      " [610000.       598937.      ]\n",
      " [ 60000.       170440.3125  ]\n",
      " ...\n",
      " [320000.       265466.21875 ]\n",
      " [ 15000.        85582.78125 ]\n",
      " [220000.       301973.      ]]\n"
     ]
    }
   ],
   "source": [
    "price_predict = model.predict(X_test)\n",
    "print(np.c_[y_test, mms.inverse_transform(price_predict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331/331 [==============================] - 2s 6ms/step - loss: 7.6372e-05 - mae: 0.0045\n",
      "mmt_test_mse = 7.637197995791212e-05, mmt_test_mae = 0.004520006477832794\n"
     ]
    }
   ],
   "source": [
    "mmt_test_mse, mmt_test_mae = model.evaluate(X_test, mmt_y_test)\n",
    "print(f\"mmt_test_mse = {mmt_test_mse}, mmt_test_mae = {mmt_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1324/1324 [==============================] - 28s 21ms/step - loss: 0.4915 - mae: 0.1615\n",
      "Epoch 2/5\n",
      "1324/1324 [==============================] - 28s 21ms/step - loss: 0.4323 - mae: 0.1443\n",
      "Epoch 3/5\n",
      "1324/1324 [==============================] - 28s 21ms/step - loss: 0.4644 - mae: 0.1363\n",
      "Epoch 4/5\n",
      "1324/1324 [==============================] - 28s 21ms/step - loss: 0.3744 - mae: 0.1310\n",
      "Epoch 5/5\n",
      "1324/1324 [==============================] - 28s 21ms/step - loss: 0.3477 - mae: 0.1308\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit(X_train, st_y_train, epochs=5, verbose=1)\n",
    "\n",
    "# st_test_mse, st_test_mae = model.evaluate(X_test, st_y_test)\n",
    "# print(f\"st_test_mse = {st_test_mse}, st_test_mae = {st_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
